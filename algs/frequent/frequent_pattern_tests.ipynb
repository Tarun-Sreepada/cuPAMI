{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'algs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01malgs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcuFPMiner_bit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cuFPMiner_bit\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01malgs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcuFPMiner_hash\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cuFPMiner_hash\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01malgs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcsv_to_parquet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m csv_to_parquet\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'algs'"
     ]
    }
   ],
   "source": [
    "from algs.cuFPMiner_bit import cuFPMiner_bit\n",
    "from algs.cuFPMiner_hash import cuFPMiner_hash\n",
    "from algs.csv_to_parquet import csv_to_parquet\n",
    "from matplotlib import pyplot as plt\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_output(output):\n",
    "    metrics = {}\n",
    "    \n",
    "    # Use regular expressions to find the relevant metrics\n",
    "    time_to_read_match = re.search(r\"Time to read:\\s*([\\d.]+)\\s*seconds\", output)\n",
    "    runtime_match = re.search(r\"Runtime:\\s*([\\d.]+)\\s*seconds\", output)\n",
    "    num_patterns_match = re.search(r\"Number of patterns:\\s*(\\d+)\", output)\n",
    "    memory_usage_match = re.search(r\"Memory usage:\\s*(\\d+)\\s*MB\", output)\n",
    "\n",
    "    time_to_read = time_to_read_match.group(1) if time_to_read_match else \"N/A\"\n",
    "    runtime = runtime_match.group(1) if runtime_match else \"N/A\"\n",
    "    num_patterns = num_patterns_match.group(1) if num_patterns_match else \"N/A\"\n",
    "    memory_usage = memory_usage_match.group(1) if memory_usage_match else \"N/A\"\n",
    "    \n",
    "    if time_to_read_match:\n",
    "        metrics['time_to_read'] = float(time_to_read_match.group(1))\n",
    "    if runtime_match:\n",
    "        metrics['runtime'] = float(runtime_match.group(1))\n",
    "    if num_patterns_match:\n",
    "        metrics['number_of_patterns'] = int(num_patterns_match.group(1))\n",
    "    if memory_usage_match:\n",
    "        metrics['memory_usage_mb'] = int(memory_usage_match.group(1))\n",
    "        \n",
    "\n",
    "    return metrics\n",
    "\n",
    "def run_alg(alg_path, file_path, minsup, separator, num_cores):\n",
    "\n",
    "    # Construct the command\n",
    "    command = [\n",
    "        alg_path, \n",
    "        file_path, \n",
    "        str(minsup), \n",
    "        separator, \n",
    "        str(num_cores), \n",
    "        \"/dev/null\"  # Discard output to /dev/null\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # Run the command and capture the output\n",
    "        result = subprocess.run(command, capture_output=True, text=True, check=True)\n",
    "        output = result.stdout\n",
    "\n",
    "        # Parse the output\n",
    "        metrics = parse_output(output)\n",
    "        return metrics\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error: Command failed with return code {e.returncode}\")\n",
    "        print(f\"Stderr: {e.stderr}\")\n",
    "        return None\n",
    "    except Exception as ex:\n",
    "        print(f\"Error: {ex}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "work = [\n",
    "        # [\"/home/tarun/cuPAMI/datasets/Transactional_retail.csv\", [100,90,80,70]],\n",
    "        # [\"/home/tarun/cuPAMI/datasets/Transactional_T10I4D100K.csv\", [50, 25, 10, 5]],\n",
    "        # [\"/home/tarun/cuPAMI/datasets/Transactional_pumsb.csv\", [41000, 40000,39000, 38000]],\n",
    "        [\"/home/tarun/cuPAMI/datasets/Transactional_chess1.csv\", [2000, 1900,1800,1700,1600]],\n",
    "        ]\n",
    "sep = \"\\t\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  col_0 col_1 col_2 col_3 col_4 col_5 col_6 col_7 col_8 col_9  ... col_27  \\\n",
      "0     1     3     5     7     9    11    13    15    17    19  ...     56   \n",
      "1     1     3     5     7     9    12    13    15    17    19  ...     56   \n",
      "2     1     3     5     7     9    12    13    16    17    19  ...     56   \n",
      "3     1     3     5     7     9    11    13    15    17    20  ...     56   \n",
      "4     1     3     5     7     9    11    13    15    17    19  ...     56   \n",
      "\n",
      "  col_28 col_29 col_30 col_31 col_32 col_33 col_34 col_35 col_36  \n",
      "0     58     60     62     64     66     68     70     72     74  \n",
      "1     58     60     62     64     66     68     70     72     74  \n",
      "2     58     60     62     64     66     68     70     72     74  \n",
      "3     58     60     62     64     66     68     70     72     74  \n",
      "4     58     60     62     64     66     68     70     72     74  \n",
      "\n",
      "[5 rows x 37 columns]\n",
      "Finished processing /home/tarun/cuPAMI/datasets/Transactional_chess1.csv\n",
      "Saved plot results/pumsb_runtime_vs_min_sup.svg\n",
      "Saved plot results/pumsb_patterns_vs_min_sup.svg\n",
      "Saved plot results/pumsb_memory_vs_min_sup.svg\n",
      "Saved plot results/pumsb_time_to_read_vs_min_sup.svg\n",
      "Saved plot results/chess1_runtime_vs_min_sup.svg\n",
      "Saved plot results/chess1_patterns_vs_min_sup.svg\n",
      "Saved plot results/chess1_memory_vs_min_sup.svg\n",
      "Saved plot results/chess1_time_to_read_vs_min_sup.svg\n",
      "Saved plot results/retail_runtime_vs_min_sup.svg\n",
      "Saved plot results/retail_patterns_vs_min_sup.svg\n",
      "Saved plot results/retail_memory_vs_min_sup.svg\n",
      "Saved plot results/retail_time_to_read_vs_min_sup.svg\n",
      "Saved plot results/T10I4D100K_runtime_vs_min_sup.svg\n",
      "Saved plot results/T10I4D100K_patterns_vs_min_sup.svg\n",
      "Saved plot results/T10I4D100K_memory_vs_min_sup.svg\n",
      "Saved plot results/T10I4D100K_time_to_read_vs_min_sup.svg\n"
     ]
    }
   ],
   "source": [
    "def clean_filename(file_path):\n",
    "    \"\"\"Clean the filename by removing everything before the last '/' and removing 'transactional' and the extension.\"\"\"\n",
    "    base_name = os.path.basename(file_path)\n",
    "    cleaned_name = base_name.replace(\"Transactional_\", \"\").split(\".\")[0]\n",
    "    return cleaned_name\n",
    "\n",
    "def run_workload(work, sep, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for file_path, min_sups in work:\n",
    "        cleaned_name = clean_filename(file_path)\n",
    "        file_output_csv = os.path.join(output_dir, f\"{cleaned_name}.csv\")\n",
    "\n",
    "        # Load existing results if the file already exists\n",
    "        if os.path.exists(file_output_csv):\n",
    "            df_existing = pd.read_csv(file_output_csv)\n",
    "            processed_combinations = set(\n",
    "                zip(df_existing[\"file\"], df_existing[\"min_sup\"], df_existing[\"algorithm\"])\n",
    "            )\n",
    "        else:\n",
    "            processed_combinations = set()\n",
    "\n",
    "        results = []\n",
    "        parquet_file = file_path.replace(\".csv\", \".parquet\")\n",
    "        csv_to_parquet(file_path, parquet_file, sep)\n",
    "        \n",
    "        for min_sup in min_sups:\n",
    "            # Define algorithms and configurations\n",
    "            algorithms = [\n",
    "                (\"cuFPMiner_bit_csv\", lambda: cuFPMiner_bit(file_path, min_sup, sep, 'csv', \"managed\")),\n",
    "                (\"cuFPMiner_hash_csv\", lambda: cuFPMiner_hash(file_path, min_sup, sep, 'csv', \"managed\")),\n",
    "                (\"cuFPMiner_hash_shared_csv\", lambda: cuFPMiner_hash(file_path, min_sup, sep, 'csv', \"managed\", True)),\n",
    "                (\"cuFPMiner_bit_parquet\", lambda: cuFPMiner_bit(parquet_file, min_sup, sep, 'parquet', \"managed\")),\n",
    "                (\"cuFPMiner_hash_parquet\", lambda: cuFPMiner_hash(parquet_file, min_sup, sep, 'parquet', \"managed\")),\n",
    "                (\"cuFPMiner_hash_shared_parquet\", lambda: cuFPMiner_hash(parquet_file, min_sup, sep, 'parquet', \"managed\", True)),\n",
    "            ]\n",
    "\n",
    "            # Run Python-based algorithms\n",
    "            for alg_name, alg_func in algorithms:\n",
    "                if (file_path, min_sup, alg_name) in processed_combinations:\n",
    "                    continue\n",
    "\n",
    "                alg = alg_func()\n",
    "                alg.mine()\n",
    "                result = {\n",
    "                    \"algorithm\": alg_name,\n",
    "                    \"file\": file_path,\n",
    "                    \"min_sup\": min_sup,\n",
    "                    \"runtime\": alg.getRuntime(),\n",
    "                    \"patterns\": len(alg.getPatterns()),\n",
    "                    \"memory\": alg.getMemoryRSS(),\n",
    "                    \"time_to_read\": alg.getTimeToRead(),\n",
    "                }\n",
    "                results.append(result)\n",
    "                pd.DataFrame([result]).to_csv(file_output_csv, mode='a', header=not os.path.exists(file_output_csv), index=False)\n",
    "                processed_combinations.add((file_path, min_sup, alg_name))\n",
    "\n",
    "            # Run command-based algorithms\n",
    "            for alg_name, command_template in [\n",
    "                # (\"apriori\", \"/home/tarun/cuPAMI/algs/apriori\"),\n",
    "                (\"fpgrowth\", \"/home/tarun/cuPAMI/algs/fpgrowth\"),\n",
    "            ]:\n",
    "                if (file_path, min_sup, alg_name) in processed_combinations:\n",
    "                    continue\n",
    "\n",
    "                metrics = run_alg(command_template, file_path, min_sup, sep, 16)\n",
    "                if metrics:\n",
    "                    result = {\n",
    "                        \"algorithm\": alg_name + \" 16 threads\",\n",
    "                        \"file\": file_path,\n",
    "                        \"min_sup\": min_sup,\n",
    "                        \"runtime\": metrics.get(\"runtime\", 0),\n",
    "                        \"patterns\": metrics.get(\"number_of_patterns\", 0),\n",
    "                        \"memory\": metrics.get(\"memory_usage_mb\", 0),\n",
    "                        \"time_to_read\": metrics.get(\"time_to_read\", 0),\n",
    "                    }\n",
    "                    results.append(result)\n",
    "                    pd.DataFrame([result]).to_csv(file_output_csv, mode='a', header=not os.path.exists(file_output_csv), index=False)\n",
    "                    processed_combinations.add((file_path, min_sup, alg_name))\n",
    "\n",
    "        print(f\"Finished processing {file_path}\")\n",
    "\n",
    "def plot_results(output_dir):\n",
    "    for csv_file in os.listdir(output_dir):\n",
    "        if not csv_file.endswith(\".csv\"):\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(output_dir, csv_file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        cleaned_name = os.path.splitext(csv_file)[0]\n",
    "\n",
    "        metrics = [\"runtime\", \"patterns\", \"memory\", \"time_to_read\"]\n",
    "        for metric in metrics:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            for alg in df[\"algorithm\"].unique():\n",
    "                alg_df = df[df[\"algorithm\"] == alg]\n",
    "                plt.plot(alg_df[\"min_sup\"], alg_df[metric], label=alg, marker=\"o\")\n",
    "\n",
    "            plt.title(f\"{metric.capitalize()} vs Minimum Support ({cleaned_name})\")\n",
    "            plt.xlabel(\"Minimum Support\")\n",
    "            plt.ylabel(metric.capitalize())\n",
    "            plt.legend()\n",
    "            plt.grid()\n",
    "\n",
    "            plot_path = os.path.join(output_dir, f\"{cleaned_name}_{metric}_vs_min_sup.svg\")\n",
    "            plt.savefig(plot_path, format=\"svg\", transparent=True)\n",
    "            plt.close()\n",
    "\n",
    "            print(f\"Saved plot {plot_path}\")\n",
    "\n",
    "# Main workflow\n",
    "output_dir = \"results\"\n",
    "run_workload(work, sep, output_dir)\n",
    "plot_results(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-24.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
