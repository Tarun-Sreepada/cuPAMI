{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuFPMiner_bit import cuFPMiner_bit\n",
    "from cuFPMiner_hash import cuFPMiner_hash\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_output(output):\n",
    "    metrics = {}\n",
    "    \n",
    "    # Use regular expressions to find the relevant metrics\n",
    "    time_to_read_match = re.search(r\"Time to read:\\s*([\\d.]+)\\s*seconds\", output)\n",
    "    runtime_match = re.search(r\"Runtime:\\s*([\\d.]+)\\s*seconds\", output)\n",
    "    num_patterns_match = re.search(r\"Number of patterns:\\s*(\\d+)\", output)\n",
    "    memory_usage_match = re.search(r\"Memory usage:\\s*(\\d+)\\s*MB\", output)\n",
    "    peak_memory_usage_match = re.search(r\"Peak memory usage:\\s*(\\d+)\\s*MB\", output)\n",
    "    \n",
    "    if time_to_read_match:\n",
    "        metrics['time_to_read'] = float(time_to_read_match.group(1))\n",
    "    if runtime_match:\n",
    "        metrics['runtime'] = float(runtime_match.group(1))\n",
    "    if num_patterns_match:\n",
    "        metrics['number_of_patterns'] = int(num_patterns_match.group(1))\n",
    "    if memory_usage_match:\n",
    "        metrics['memory_usage_mb'] = int(memory_usage_match.group(1))\n",
    "    if peak_memory_usage_match:\n",
    "        metrics['peak_memory_usage_mb'] = int(peak_memory_usage_match.group(1))\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def run_alg(alg_path, file_path, minsup, separator, num_cores):\n",
    "\n",
    "    # Construct the command\n",
    "    command = [\n",
    "        alg_path, \n",
    "        file_path, \n",
    "        str(minsup), \n",
    "        separator, \n",
    "        str(num_cores), \n",
    "        \"/dev/null\"  # Discard output to /dev/null\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # Run the command and capture the output\n",
    "        result = subprocess.run(command, capture_output=True, text=True, check=True)\n",
    "        output = result.stdout\n",
    "\n",
    "        # Parse the output\n",
    "        metrics = parse_output(output)\n",
    "        return metrics\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error: Command failed with return code {e.returncode}\")\n",
    "        print(f\"Stderr: {e.stderr}\")\n",
    "        return None\n",
    "    except Exception as ex:\n",
    "        print(f\"Error: {ex}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "work = [\n",
    "        # [\"../../datasets/transactional/Transactional_retail.csv\", [100,90,80,70]],\n",
    "        # [\"../../datasets/transactional/Transactional_T10I4D100K.csv\", [50, 25, 10, 5]],\n",
    "        [\"../../datasets/transactional/Transactional_pumsb.csv\", [41000, 40000,39000, 38000]],\n",
    "        [\"../../datasets/transactional/Transactional_chess1.csv\", [2000, 1900,1800,1700,1600]],\n",
    "        ]\n",
    "sep = \"\\t\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_filename(file_path):\n",
    "    \"\"\"Clean the filename by removing everything before the last '/' and removing 'transactional' and the extension.\"\"\"\n",
    "    base_name = os.path.basename(file_path)\n",
    "    cleaned_name = base_name.replace(\"Transactional_\", \"\").split(\".\")[0]\n",
    "    return cleaned_name\n",
    "\n",
    "def run_workload(work, sep, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for file_path, min_sups in work:\n",
    "        cleaned_name = clean_filename(file_path)\n",
    "        file_output_csv = os.path.join(output_dir, f\"{cleaned_name}.csv\")\n",
    "\n",
    "        # Load existing results if the file already exists\n",
    "        if os.path.exists(file_output_csv):\n",
    "            df_existing = pd.read_csv(file_output_csv)\n",
    "            processed_combinations = set(\n",
    "                zip(df_existing[\"file\"], df_existing[\"min_sup\"], df_existing[\"algorithm\"])\n",
    "            )\n",
    "        else:\n",
    "            processed_combinations = set()\n",
    "\n",
    "        results = []\n",
    "        \n",
    "        for min_sup in min_sups:\n",
    "            # Define algorithms and configurations\n",
    "            algorithms = [\n",
    "                (\"cuFPMiner_bit_csv\", lambda: cuFPMiner_bit(file_path, min_sup, sep, 'csv', \"managed\")),\n",
    "                (\"cuFPMiner_hash_csv\", lambda: cuFPMiner_hash(file_path, min_sup, sep, 'csv', \"managed\")),\n",
    "                (\"cuFPMiner_hash_shared_csv\", lambda: cuFPMiner_hash(file_path, min_sup, sep, 'csv', \"managed\", True))\n",
    "            ]\n",
    "\n",
    "            # Run Python-based algorithms\n",
    "            for alg_name, alg_func in algorithms:\n",
    "                if (file_path, min_sup, alg_name) in processed_combinations:\n",
    "                    continue\n",
    "\n",
    "                alg = alg_func()\n",
    "                alg.mine()\n",
    "                result = {\n",
    "                    \"algorithm\": alg_name,\n",
    "                    \"file\": file_path,\n",
    "                    \"min_sup\": min_sup,\n",
    "                    \"runtime\": alg.getRuntime(),\n",
    "                    \"patterns\": len(alg.getPatterns()),\n",
    "                    \"memory\": alg.getMemoryRSS(),\n",
    "                    \"time_to_read\": alg.getTimeToRead(),\n",
    "                }\n",
    "                results.append(result)\n",
    "                pd.DataFrame([result]).to_csv(file_output_csv, mode='a', header=not os.path.exists(file_output_csv), index=False)\n",
    "                processed_combinations.add((file_path, min_sup, alg_name))\n",
    "\n",
    "            # Run command-based algorithms\n",
    "            for alg_name, command_template in [\n",
    "                (\"apriori\", \"./apriori\"),\n",
    "                (\"fpgrowth\", \"./fpgrowth\"),\n",
    "            ]:\n",
    "                if (file_path, min_sup, alg_name) in processed_combinations:\n",
    "                    continue\n",
    "\n",
    "                metrics = run_alg(command_template, file_path, min_sup, sep, 16)\n",
    "                print(metrics)\n",
    "                if metrics:\n",
    "                    result = {\n",
    "                        \"algorithm\": alg_name + \" 16 threads\",\n",
    "                        \"file\": file_path,\n",
    "                        \"min_sup\": min_sup,\n",
    "                        \"runtime\": metrics.get(\"runtime\", 0),\n",
    "                        \"patterns\": metrics.get(\"number_of_patterns\", 0),\n",
    "                        \"memory\": metrics.get(\"peak_memory_usage_mb\", 0),\n",
    "                        \"time_to_read\": metrics.get(\"time_to_read\", 0),\n",
    "                    }\n",
    "                    results.append(result)\n",
    "                    pd.DataFrame([result]).to_csv(file_output_csv, mode='a', header=not os.path.exists(file_output_csv), index=False)\n",
    "                    processed_combinations.add((file_path, min_sup, alg_name))\n",
    "\n",
    "        print(f\"Finished processing {file_path}\")\n",
    "\n",
    "# Main workflow\n",
    "output_dir = \"../../results/frequent_patterns/\"\n",
    "run_workload(work, sep, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_results(output_dir):\n",
    "    for csv_file in os.listdir(output_dir):\n",
    "        if not csv_file.endswith(\".csv\"):\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(output_dir, csv_file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        cleaned_name = os.path.splitext(csv_file)[0]\n",
    "\n",
    "        metrics = [\"runtime\", \"patterns\", \"memory\"]\n",
    "        for metric in metrics:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            for alg in df[\"algorithm\"].unique():\n",
    "                alg_df = df[df[\"algorithm\"] == alg]\n",
    "                plt.plot(alg_df[\"min_sup\"], alg_df[metric], label=alg, marker=\"o\")\n",
    "\n",
    "            plt.title(f\"{metric.capitalize()} vs Minimum Support ({cleaned_name})\")\n",
    "            plt.xlabel(\"Minimum Support\")\n",
    "            plt.ylabel(metric.capitalize())\n",
    "            plt.legend()\n",
    "            plt.grid()\n",
    "\n",
    "            plot_path = os.path.join(output_dir, f\"{cleaned_name}_{metric}_vs_min_sup.svg\")\n",
    "            plt.savefig(plot_path, format=\"svg\", transparent=True)\n",
    "            plt.close()\n",
    "\n",
    "            print(f\"Saved plot {plot_path}\")\n",
    "\n",
    "# plot_results(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-24.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
